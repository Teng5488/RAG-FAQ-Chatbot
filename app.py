import os
import re
import bs4
import torch
import streamlit as st
from typing import List

from langchain_community.document_loaders import WebBaseLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document

# ============================================
# 0. ç¶²é è¨­å®š & CSS ç¾å­¸ (PLUS ULTRA STYLE)
# ============================================
st.set_page_config(
    page_title="U.A. Database | é›„è‹±è³‡æ–™åº«",
    page_icon="ğŸ¦¸â€â™‚ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ³¨å…¥ Google Fonts å’Œè‡ªå®šç¾© CSS
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Bangers&family=Noto+Sans+TC:wght@400;700&display=swap');

    /* å…¨å±€èƒŒæ™¯èˆ‡å­—é«” */
    .stApp {
        background-color: #f0f2f6;
        font-family: 'Noto Sans TC', sans-serif;
    }

    /* æ¨™é¡Œç‰¹æ•ˆ (ç¾æ¼«é¢¨æ ¼) */
    h1 {
        font-family: 'Bangers', cursive;
        color: #d32f2f;
        text-transform: uppercase;
        font-size: 3.5rem !important;
        text-shadow: 3px 3px 0px #FBC02D;
        letter-spacing: 2px;
        margin-bottom: 0px;
    }
    
    .subtitle {
        color: #1565C0;
        font-weight: bold;
        font-size: 1.2rem;
        margin-bottom: 2rem;
        border-bottom: 3px solid #FBC02D;
        display: inline-block;
        padding-bottom: 5px;
    }

    /* å´é‚Šæ¬„å„ªåŒ– (é«˜ç§‘æŠ€æ·±è‰²é¢¨) */
    section[data-testid="stSidebar"] {
        background-color: #1a1a1a;
        color: white;
    }
    section[data-testid="stSidebar"] h1, section[data-testid="stSidebar"] h2, section[data-testid="stSidebar"] h3 {
        color: #FBC02D !important; /* é‡‘é»ƒè‰²æ¨™é¡Œ */
    }
    section[data-testid="stSidebar"] span {
        color: #e0e0e0 !important;
    }

    /* èŠå¤©æ°£æ³¡å„ªåŒ– */
    .stChatMessage {
        border-radius: 20px;
        padding: 1.5rem;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        transition: transform 0.2s;
        border: 2px solid transparent;
    }
    .stChatMessage:hover {
        transform: translateY(-2px);
    }

    /* AI (Assistant) æ°£æ³¡ - æ­çˆ¾éº¥ç‰¹é…è‰² */
    div[data-testid="chatAvatarIcon-assistant"] {
        background-color: #d32f2f !important;
    }
    div[data-testid="stChatMessage"]:nth-child(even) {
        background: linear-gradient(135deg, #ffffff 0%, #fff8e1 100%);
        border-left: 5px solid #d32f2f;
    }

    /* User æ°£æ³¡ - é›„è‹±åˆ¶æœé…è‰² */
    div[data-testid="chatAvatarIcon-user"] {
        background-color: #1565C0 !important;
    }
    div[data-testid="stChatMessage"]:nth-child(odd) {
        background: linear-gradient(135deg, #e3f2fd 0%, #ffffff 100%);
        border-right: 5px solid #1565C0;
    }

    /* æŒ‰éˆ•ç¾åŒ– */
    .stButton>button {
        background: linear-gradient(90deg, #d32f2f, #b71c1c);
        color: white;
        border-radius: 30px;
        border: none;
        font-family: 'Bangers', cursive;
        font-size: 1.2rem;
        letter-spacing: 1px;
        box-shadow: 0 4px 0 #7f0000; /* ç«‹é«”æ„Ÿ */
        transition: all 0.1s;
    }
    .stButton>button:active {
        box-shadow: 0 0 0 #7f0000;
        transform: translateY(4px);
    }
    
    /* æª¢ç´¢ä¾†æºå¡ç‰‡ */
    .source-card {
        background-color: white;
        padding: 15px;
        border-radius: 10px;
        border-left: 4px solid #FBC02D;
        margin-bottom: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        font-size: 0.9rem;
    }
    
    /* éš±è—é è¨­é¸å–® */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
</style>
""", unsafe_allow_html=True)

# ============================================
# 1. æ ¸å¿ƒå‡½æ•¸ (ä¿æŒä½ çš„é‚è¼¯ä¸è®Š)
# ============================================

@st.cache_resource
def get_device():
    if torch.cuda.is_available(): return "cuda"
    return "cpu"

@st.cache_resource
def get_embeddings(device):
    return HuggingFaceEmbeddings(
        model_name="intfloat/multilingual-e5-large",
        model_kwargs={"device": device},
        encode_kwargs={"normalize_embeddings": True}
    )

def split_by_character_and_inject_context(text: str) -> List[Document]:
    text = re.sub(r'\[ç·¨è¼¯\]|\[edit\]|\[\d+\]', '', text)
    character_pattern = r'\n(?=[\u4e00-\u9fa5]{2,}[ï¼ˆ\(][^ï¼‰\)]+[ï¼‰\)])'
    sections = re.split(character_pattern, text)
    
    final_docs = []
    sub_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

    for section in sections:
        section = section.strip()
        if not section: continue
        lines = section.split('\n')
        try:
            char_header = lines[0].split('ï¼')[0].split('ï¼ˆ')[0].strip()
        except:
            char_header = "æ©Ÿå¯†æª”æ¡ˆ"
        
        for chunk in sub_splitter.split_text(section):
            final_docs.append(Document(
                page_content=f"ã€è‹±é›„æª”æ¡ˆ: {char_header}ã€‘\n{chunk}",
                metadata={"character": char_header}
            ))
    return final_docs

@st.cache_resource
def initialize_vectorstore(_embeddings):
    index_path = "./faiss_index_hero_v3"
    
    if os.path.exists(index_path):
        return FAISS.load_local(index_path, _embeddings, allow_dangerous_deserialization=True)
    
    with st.status("ğŸ“¡ æ­£åœ¨é€£æ¥é›„è‹±ä¼ºæœå™¨...", expanded=True) as status:
        st.write("æ­£åœ¨å¾ç¶­åŸºç™¾ç§‘æå–è‹±é›„æ•¸æ“š...")
        url = "https://zh.wikipedia.org/wiki/%E6%88%91%E7%9A%84%E8%8B%B1%E9%9B%84%E5%AD%A6%E9%99%A2%E8%A7%92%E8%89%B2%E5%88%97%E8%A1%A8"
        loader = WebBaseLoader(url, bs_kwargs=dict(parse_only=bs4.SoupStrainer(id="mw-content-text")))
        raw_docs = loader.load()
        
        st.write("æ­£åœ¨é€²è¡Œæ•¸æ“šå‘é‡åŒ–...")
        splits = split_by_character_and_inject_context(raw_docs[0].page_content)
        vectorstore = FAISS.from_documents(splits, _embeddings)
        vectorstore.save_local(index_path)
        status.update(label="âœ… è³‡æ–™åº«åŒæ­¥å®Œæˆï¼", state="complete", expanded=False)
        return vectorstore

# ============================================
# 2. å´é‚Šæ¬„ (Hero Support Item Interface)
# ============================================
with st.sidebar:
    st.image("assets/My_Hero_Academia_logo.png", use_container_width=True)
    st.markdown("<div style='text-align: center; color: #aaa; margin-bottom: 20px;'>SECURE TERMINAL V.3.1</div>", unsafe_allow_html=True)
    
    device = get_device()
    st.markdown(f"**ğŸŸ¢ é‹ç®—æ ¸å¿ƒç‹€æ…‹:** `{device.upper()}`")
    if device == "cuda":
        st.caption(f"ğŸš€ GPU: {torch.cuda.get_device_name(0)}")
    
    st.markdown("---")
    st.markdown("### ğŸ› ï¸ åƒæ•¸è¨­å®š")
    
    model_name = st.selectbox("Model", ["deepseek-r1:8b", "llama3"], index=0)
    temperature = st.slider("æ€ç¶­ç™¼æ•£åº¦ (Temp)", 0.0, 1.0, 0.2)
    k_retrieval = st.slider("è³‡æ–™èª¿é–±æ¬Šé™ (Docs)", 1, 10, 4)
    
    st.markdown("---")
    col_reset, col_help = st.columns(2)
    with col_reset:
        if st.button("ğŸ”„ é‡å•Ÿç³»çµ±"):
            st.session_state.messages = []
            st.rerun()

# ============================================
# 3. ä¸»ä»‹é¢ (Hero Interface)
# ============================================

# åˆå§‹åŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# Header å€åŸŸ
col_logo, col_title = st.columns([1, 6])
with col_logo:
    # é€™è£¡å¯ä»¥ç”¨ font-awesome æˆ–è€…çœŸæ­£çš„åœ–ç‰‡
    st.markdown("<div style='font-size: 60px; text-align: center;'>ğŸ¦¸â€â™‚ï¸</div>", unsafe_allow_html=True)
with col_title:
    st.markdown("<h1>U.A. HIGH DATABASE</h1>", unsafe_allow_html=True)
    st.markdown("<div class='subtitle'>æˆ‘çš„è‹±é›„å­¸é™¢è‹±é›„çŸ¥è­˜å•ç­”ç³»çµ± / PLUS ULTRA !!</div>", unsafe_allow_html=True)

# è¼‰å…¥ç³»çµ±
embeddings = get_embeddings(device)
vectorstore = initialize_vectorstore(embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": k_retrieval})

# é¡¯ç¤ºå°è©±æ­·å²
for message in st.session_state.messages:
    avatar = "ğŸ§‘â€ğŸ“" if message["role"] == "user" else "ğŸ¦¸â€â™‚ï¸"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])

# è¼¸å…¥å€
if query := st.chat_input("è«‹è¼¸å…¥å•é¡Œ"):
    
    # 1. ä½¿ç”¨è€…è¼¸å…¥
    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸ“"):
        st.markdown(query)

    # 2. æª¢ç´¢éç¨‹ (ä½¿ç”¨æŠ˜ç–Šé¸å–®ï¼Œä¿æŒä»‹é¢æ•´æ½”)
    with st.status("ğŸ” æ­£åœ¨æª¢ç´¢æ©Ÿå¯†æª”æ¡ˆ...", expanded=False) as status:
        docs = retriever.invoke(query)
        context_text = "\n\n".join([d.page_content for d in docs])
        
        # é¡¯ç¤ºæ¼‚äº®çš„ä¾†æºå¡ç‰‡
        st.markdown("### ğŸ“‚ æª¢ç´¢åˆ°çš„æª”æ¡ˆç‰‡æ®µ")
        for i, doc in enumerate(docs):
            char_name = doc.metadata.get('character', 'æœªçŸ¥')
            content_preview = doc.page_content.replace(f"ã€è‹±é›„æª”æ¡ˆ: {char_name}ã€‘", "").strip()[:100]
            st.markdown(f"""
            <div class="source-card">
                <b>#{i+1} æª”æ¡ˆä¾†æº: {char_name}</b><br>
                <span style="color: #666;">{content_preview}...</span>
            </div>
            """, unsafe_allow_html=True)
        status.update(label=f"âœ… æª¢ç´¢å®Œæˆï¼å…±ç™¼ç¾ {len(docs)} ç­†é—œè¯è³‡æ–™", state="complete")

    # 3. AI å›ç­”
    with st.chat_message("assistant", avatar="ğŸ¦¸â€â™‚ï¸"):
        response_placeholder = st.empty()
        
        # Prompt
        llm = ChatOllama(model=model_name, temperature=temperature)
        prompt_template = ChatPromptTemplate.from_template(

            """è«‹æ ¹æ“šæä¾›çš„è³‡æ–™å›ç­”å•é¡Œã€‚
            è³‡æ–™ï¼š{context}
            å•é¡Œï¼š{question}
            å›ç­”ï¼š
            å›ç­”è¦å‰‡ï¼š
            1. æ¢åˆ—å¼å‘ˆç¾é‡é»ã€‚
            2. ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚
            """
        )
        
        chain = (
            {"context": lambda x: context_text, "question": RunnablePassthrough()}
            | prompt_template
            | llm
            | StrOutputParser()
        )
        
        # ä¸²æµè¼¸å‡º + DeepSeek æ€è€ƒåˆ†é›¢
        full_response = ""
        final_answer_buffer = ""
        thought_buffer = ""
        is_thinking = False
        
        # å»ºç«‹æ€è€ƒå€å¡Š (å¦‚æœæ¨¡å‹æ”¯æ´)
        thought_expander = None 
        
        try:
            for chunk in chain.stream(query):
                full_response += chunk
                
                # è™•ç† <think> æ¨™ç±¤
                if "<think>" in chunk:
                    is_thinking = True
                    thought_expander = st.expander("ğŸ§  æˆ°è¡“åˆ†æéç¨‹ (DeepSeek)", expanded=True)
                    chunk = chunk.replace("<think>", "")
                
                if "</think>" in chunk:
                    is_thinking = False
                    chunk = chunk.replace("</think>", "")
                    
                if is_thinking and thought_expander:
                    thought_buffer += chunk
                    thought_expander.markdown(f"_{thought_buffer}_")
                else:
                    final_answer_buffer += chunk
                    response_placeholder.markdown(final_answer_buffer + "â–Œ")
            
            # æœ€å¾Œé¡¯ç¤ºå®Œæ•´æ–‡å­— (ç§»é™¤æ¸¸æ¨™)
            response_placeholder.markdown(final_answer_buffer)
            
        except Exception as e:
            st.error(f"âŒ ç³»çµ±éŒ¯èª¤: è«‹ç¢ºèª Ollama æ˜¯å¦å·²å•Ÿå‹• ({e})")

    # 4. å­˜æª”
    st.session_state.messages.append({"role": "assistant", "content": final_answer_buffer})